import { OpenAIEmbeddings, OpenAI } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { Document } from "langchain/document";
import { RetrievalQAChain } from "langchain/chains";

const model = new OpenAI({ model: "gpt-4o-mini", temperature: 0 });
const embeddings = new OpenAIEmbeddings();

// ğŸ§  NaÅ¡a mala baza znanja (koju LLM sam po sebi ne zna)
const docs = [
    new Document({ pageContent: "Project Falcon uses PostgreSQL for database and Redis for caching." }),
    new Document({ pageContent: "The internal API for Falcon is deployed on AWS Lambda." }),
];

// ğŸ”¹ 1ï¸âƒ£ Pretvori dokumente u embeddinge i napravi vector store
const vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);

// ğŸ”¹ 2ï¸âƒ£ Retriever dohvaÄ‡a najrelevantnije dokumente
const retriever = vectorStore.asRetriever(2);

// ğŸ”¹ 3ï¸âƒ£ Kreiraj RAG lanac (spaja retrieval + generation)
const chain = RetrievalQAChain.fromLLM(model, retriever, {
    returnSourceDocuments: true,
});

// ğŸ”¹ 4ï¸âƒ£ Upit koji LLM sam po sebi ne bi znao
const query = "What database does Project Falcon use?";
const result = await chain.invoke({ query });

console.log("Q:", query);
console.log("A:", result.text);
console.log("Context used:", result.sourceDocuments.map(d => d.pageContent));
